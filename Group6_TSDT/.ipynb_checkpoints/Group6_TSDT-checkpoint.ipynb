{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e142b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group Number : 6\n",
    "# Roll Numbers : 19EC37002, 20CS10047\n",
    "# Project Number : Khyathi Nalluri, Priyanshi Dixit\n",
    "# Project Title : [ Project Code: TSDT ] - Predicting Sinking Titanic Survivors using Decision Tree based Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9c4fb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[101  15]\n",
      " [ 15  48]]\n",
      "Accuracy: \n",
      "83.24022346368714\n",
      "Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87       116\n",
      "           1       0.76      0.76      0.76        63\n",
      "\n",
      "    accuracy                           0.83       179\n",
      "   macro avg       0.82      0.82      0.82       179\n",
      "weighted avg       0.83      0.83      0.83       179\n",
      "\n",
      "Confusion Matrix: \n",
      "[[103  13]\n",
      " [ 23  39]]\n",
      "Accuracy: \n",
      "79.7752808988764\n",
      "Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.85       116\n",
      "           1       0.75      0.63      0.68        62\n",
      "\n",
      "    accuracy                           0.80       178\n",
      "   macro avg       0.78      0.76      0.77       178\n",
      "weighted avg       0.79      0.80      0.79       178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00,  6.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[100   9]\n",
      " [ 25  44]]\n",
      "Accuracy: \n",
      "80.89887640449437\n",
      "Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.92      0.85       109\n",
      "           1       0.83      0.64      0.72        69\n",
      "\n",
      "    accuracy                           0.81       178\n",
      "   macro avg       0.82      0.78      0.79       178\n",
      "weighted avg       0.81      0.81      0.80       178\n",
      "\n",
      "Confusion Matrix: \n",
      "[[90 15]\n",
      " [21 52]]\n",
      "Accuracy: \n",
      "79.7752808988764\n",
      "Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.83       105\n",
      "           1       0.78      0.71      0.74        73\n",
      "\n",
      "    accuracy                           0.80       178\n",
      "   macro avg       0.79      0.78      0.79       178\n",
      "weighted avg       0.80      0.80      0.80       178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[92 11]\n",
      " [23 52]]\n",
      "Accuracy: \n",
      "80.89887640449437\n",
      "Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.89      0.84       103\n",
      "           1       0.83      0.69      0.75        75\n",
      "\n",
      "    accuracy                           0.81       178\n",
      "   macro avg       0.81      0.79      0.80       178\n",
      "weighted avg       0.81      0.81      0.81       178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[101  15]\n",
      " [ 14  49]]\n",
      "Accuracy: \n",
      "83.79888268156425\n",
      "Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.87       116\n",
      "           1       0.77      0.78      0.77        63\n",
      "\n",
      "    accuracy                           0.84       179\n",
      "   macro avg       0.82      0.82      0.82       179\n",
      "weighted avg       0.84      0.84      0.84       179\n",
      "\n",
      "accuracies of Trees corresponding to 5 data splits :\n",
      "tree-1 : 0.832\n",
      "tree-2 : 0.798\n",
      "tree-3 : 0.809\n",
      "tree-4 : 0.798\n",
      "tree-5 : 0.809\n",
      "tree with best accuracy - 0.8324 :\n",
      "{'Sex <= 0.5': [{'Pclass <= 2.5': [{'Age <= 2.5': [{'Parch <= 1.5': [1.0, 0.0]},\n",
      "                                                   1.0]},\n",
      "                                   {'Age <= 39.0': [{'SibSp <= 2.5': [{'Age <= 1.5': [1.0,\n",
      "                                                                                      {'Age <= 3.0': [0.0,\n",
      "                                                                                                      1.0]}]},\n",
      "                                                                      {'Parch <= 0.5': [1.0,\n",
      "                                                                                        0.0]}]},\n",
      "                                                    0.0]}]},\n",
      "                {'Pclass <= 1.5': [{'Age <= 60.5': [{'Age <= 17.5': [1.0,\n",
      "                                                                     {'Age <= 22.5': [0.0,\n",
      "                                                                                      {'Age <= 27.5': [1.0,\n",
      "                                                                                                       0.0]}]}]},\n",
      "                                                    0.0]},\n",
      "                                   {'Age <= 13.0': [{'SibSp <= 2.5': [{'Parch <= 0.5': [{'SibSp <= 0.5': [0.0,\n",
      "                                                                                                          1.0]},\n",
      "                                                                                        1.0]},\n",
      "                                                                      0.0]},\n",
      "                                                    0.0]}]}]}\n",
      "best tree with best accuracy - 0.83 after pruning [pruned accuracy - 0.84] :\n",
      "{'Sex <= 0.5': [1,\n",
      "                {'Pclass <= 1.5': [0,\n",
      "                                   {'Age <= 13.0': [{'SibSp <= 2.5': [1, 0.0]},\n",
      "                                                    0.0]}]}]}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import csv # data processing, CSV file I/O\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier # sklearn Decision Tree\n",
    "import operator\n",
    "from math import log\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class decision_tree():\n",
    "    def __init__(self, train, test, prune=False):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.prune = prune\n",
    "        self.column_headers = self.train.columns\n",
    "        self.tree = self.decision_tree_algorithm_igain(self.train)\n",
    "        self.accuracy = self.calculate_accuracy(self.tree)\n",
    "        if prune :\n",
    "            self.pruned_tree = self.post_pruning(self.tree, self.train, self.test)\n",
    "            self.pruned_accuracy = self.calculate_accuracy(self.pruned_tree)\n",
    "\n",
    "\n",
    "    def check_purity(self,data):\n",
    "        label_column = data[:, -1]\n",
    "        unique_classes = np.unique(label_column)\n",
    "\n",
    "        if len(unique_classes) == 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def classify_data(self,data):\n",
    "        label_column = data[:, -1]\n",
    "        unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "\n",
    "        index = counts_unique_classes.argmax()\n",
    "        classification = unique_classes[index]\n",
    "\n",
    "        return classification\n",
    "\n",
    "    def get_potential_splits(self,data):\n",
    "        potential_splits = {}\n",
    "        _, n_columns = data.shape\n",
    "        for column_index in range(n_columns - 1):  # excluding the last column which is the label\n",
    "            potential_splits[column_index] = []\n",
    "            values = data[:, column_index]\n",
    "            unique_values = np.unique(values)\n",
    "\n",
    "            for index in range(len(unique_values)):\n",
    "                if index != 0:\n",
    "                    current_value = unique_values[index]\n",
    "                    previous_value = unique_values[index - 1]\n",
    "                    potential_split = (current_value + previous_value) / 2\n",
    "\n",
    "                    potential_splits[column_index].append(potential_split)\n",
    "\n",
    "        return potential_splits\n",
    "\n",
    "    def split_data(self,data, split_column, split_value):\n",
    "        split_column_values = data[:, split_column]\n",
    "\n",
    "        data_below = data[split_column_values <= split_value]\n",
    "        data_above = data[split_column_values > split_value]\n",
    "\n",
    "        return data_below, data_above\n",
    "\n",
    "    def calculate_entropy(self,data):\n",
    "        label_column = data[:, -1]\n",
    "        _, counts = np.unique(label_column, return_counts=True)\n",
    "\n",
    "        probabilities = counts / counts.sum()\n",
    "        entropy = sum(probabilities * -np.log2(probabilities))\n",
    "\n",
    "        return entropy\n",
    "\n",
    "    def calculate_overall_entropy(self,data_below, data_above):\n",
    "        n = len(data_below) + len(data_above)\n",
    "        p_data_below = len(data_below) / n\n",
    "        p_data_above = len(data_above) / n\n",
    "\n",
    "        overall_entropy = (p_data_below * self.calculate_entropy(data_below)\n",
    "                           + p_data_above * self.calculate_entropy(data_above))\n",
    "\n",
    "        return overall_entropy\n",
    "\n",
    "    def determine_best_split_igain(self,data, potential_splits):\n",
    "        overall_entropy = 9999\n",
    "        for column_index in potential_splits:\n",
    "            for value in potential_splits[column_index]:\n",
    "                data_below, data_above = self.split_data(data, split_column=column_index, split_value=value)\n",
    "                current_overall_entropy =self.calculate_overall_entropy(data_below, data_above)\n",
    "\n",
    "                if current_overall_entropy <= overall_entropy:\n",
    "                    overall_entropy = current_overall_entropy\n",
    "                    best_split_column = column_index\n",
    "                    best_split_value = value\n",
    "\n",
    "        return best_split_column, best_split_value\n",
    "\n",
    "    def decision_tree_algorithm_igain(self, df, counter=0, min_samples=2, max_depth=6):\n",
    "        # data preparations\n",
    "        if counter == 0:\n",
    "            data = df.values\n",
    "        else:\n",
    "            data = df\n",
    "\n",
    "            # base cases\n",
    "        if (self.check_purity(data)) or (len(data) < min_samples) or (counter == max_depth):\n",
    "            classification = self.classify_data(data)\n",
    "\n",
    "            return classification\n",
    "\n",
    "        # recursive part\n",
    "        else:\n",
    "            counter += 1\n",
    "            # helper functions\n",
    "            potential_splits = self.get_potential_splits(data)\n",
    "            split_column, split_value = self.determine_best_split_igain(data, potential_splits)\n",
    "            data_below, data_above = self.split_data(data, split_column, split_value)\n",
    "            # instantiate sub-tree\n",
    "            feature_name = self.column_headers[split_column]\n",
    "            question = \"{} <= {}\".format(feature_name, split_value)\n",
    "            sub_tree = {question: []}\n",
    "\n",
    "            # find answers (recursion)\n",
    "            yes_answer = self.decision_tree_algorithm_igain(data_below, counter, min_samples, max_depth)\n",
    "            no_answer = self.decision_tree_algorithm_igain(data_above, counter, min_samples, max_depth)\n",
    "\n",
    "            # If the answers are the same, then there is no point in asking the qestion.\n",
    "            # This could happen when the data is classified even though it is not pure\n",
    "            # yet (min_samples or max_depth base cases).\n",
    "            if yes_answer == no_answer:\n",
    "                sub_tree = yes_answer\n",
    "            else:\n",
    "                sub_tree[question].append(yes_answer)\n",
    "                sub_tree[question].append(no_answer)\n",
    "\n",
    "            return sub_tree\n",
    "\n",
    "    def calculate_accuracy(self, tree):\n",
    "\n",
    "        def classify_example(example, tree):\n",
    "            question = list(tree.keys())[0]\n",
    "            feature_name, comparison_operator, value = question.split(\" \")\n",
    "\n",
    "            # ask question\n",
    "            if comparison_operator == \"<=\":  # feature is continuous\n",
    "                if example[feature_name] <= float(value):\n",
    "                    answer = tree[question][0]\n",
    "                else:\n",
    "                    answer = tree[question][1]\n",
    "\n",
    "            # feature is categorical\n",
    "            else:\n",
    "                if str(example[feature_name]) == value:\n",
    "                    answer = tree[question][0]\n",
    "                else:\n",
    "                    answer = tree[question][1]\n",
    "\n",
    "            # base case\n",
    "            if not isinstance(answer, dict):\n",
    "                return answer\n",
    "\n",
    "            # recursive part\n",
    "            else:\n",
    "                residual_tree = answer\n",
    "                return classify_example(example, residual_tree)\n",
    "\n",
    "        adf = self.test.copy(deep=True)\n",
    "        adf.insert(1, \"classification\", adf.apply(classify_example, axis=1, args=(tree, )).to_list())\n",
    "        adf.insert(1, \"classification_correct\", list(adf[\"classification\"] == adf[\"label\"]))\n",
    "        accuracy = adf[\"classification_correct\"].mean()\n",
    "        \n",
    "        print(\"Confusion Matrix: \")\n",
    "        print(confusion_matrix(adf['label'], adf['classification']))\n",
    "    \n",
    "        print(\"Accuracy: \")\n",
    "        print(accuracy*100)\n",
    "    \n",
    "        print(\"Report: \")\n",
    "        print(classification_report(adf['label'], adf['classification']))\n",
    "        \n",
    "        return accuracy\n",
    "\n",
    "    # pruning methods\n",
    "\n",
    "    def make_predictions(self, df, tree):\n",
    "\n",
    "        def predict_example(example, tree):\n",
    "            # tree is just a root node\n",
    "            if not isinstance(tree, dict):\n",
    "                return tree\n",
    "\n",
    "            question = list(tree.keys())[0]\n",
    "            feature_name, comparison_operator, value = question.split(\" \")\n",
    "\n",
    "            # ask question\n",
    "            if comparison_operator == \"<=\":\n",
    "                if example[feature_name] <= float(value):\n",
    "                    answer = tree[question][0]\n",
    "                else:\n",
    "                    answer = tree[question][1]\n",
    "\n",
    "            # feature is categorical\n",
    "            else:\n",
    "                if str(example[feature_name]) == value:\n",
    "                    answer = tree[question][0]\n",
    "                else:\n",
    "                    answer = tree[question][1]\n",
    "\n",
    "            # base case\n",
    "            if not isinstance(answer, dict):\n",
    "                return answer\n",
    "\n",
    "            # recursive part\n",
    "            else:\n",
    "                residual_tree = answer\n",
    "                return predict_example(example, residual_tree)\n",
    "\n",
    "        if len(df) != 0:\n",
    "            predictions = df.apply(predict_example, args=(tree,), axis=1)\n",
    "        else:\n",
    "            # \"df.apply()\"\" with empty dataframe returns an empty dataframe,\n",
    "            # but \"predictions\" should be a series instead\n",
    "            predictions = pd.Series(dtype='float64')\n",
    "            \n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def filter_df(self, df, question):\n",
    "        feature, comparison_operator, value = question.split()\n",
    "\n",
    "        # continuous feature\n",
    "        if comparison_operator == \"<=\":\n",
    "            df_yes = df[df[feature] <= float(value)]\n",
    "            df_no = df[df[feature] > float(value)]\n",
    "\n",
    "        # categorical feature\n",
    "        else:\n",
    "            df_yes = df[df[feature].astype(str) == value]\n",
    "            df_no = df[df[feature].astype(str) != value]\n",
    "\n",
    "        return df_yes, df_no\n",
    "\n",
    "    def determine_leaf(self, df_train):\n",
    "        return df_train.label.value_counts().index[0]\n",
    "\n",
    "    def determine_errors(self, df_val, tree):\n",
    "        predictions = self.make_predictions(df_val, tree)\n",
    "        actual_values = df_val.label\n",
    "        return sum(predictions != actual_values)\n",
    "\n",
    "    def pruning_result(self, tree, df_train, df_val):\n",
    "        leaf = self.determine_leaf(df_train)\n",
    "        errors_leaf = self.determine_errors(df_val, leaf)\n",
    "        errors_decision_node = self.determine_errors(df_val, tree)\n",
    "\n",
    "        if errors_leaf <= errors_decision_node:\n",
    "            return leaf\n",
    "        else:\n",
    "            return tree\n",
    "\n",
    "    def post_pruning(self, tree, df_train, df_val):\n",
    "        question = list(tree.keys())[0]\n",
    "        yes_answer, no_answer = tree[question]\n",
    "\n",
    "        # base case\n",
    "        if not isinstance(yes_answer, dict) and not isinstance(no_answer, dict):\n",
    "            return self.pruning_result(tree, df_train, df_val)\n",
    "\n",
    "        # recursive part\n",
    "        else:\n",
    "            df_train_yes, df_train_no = self.filter_df(df_train, question)\n",
    "            df_val_yes, df_val_no = self.filter_df(df_val, question)\n",
    "\n",
    "            if isinstance(yes_answer, dict):\n",
    "                yes_answer = self.post_pruning(yes_answer, df_train_yes, df_val_yes)\n",
    "\n",
    "            if isinstance(no_answer, dict):\n",
    "                no_answer = self.post_pruning(no_answer, df_train_no, df_val_no)\n",
    "\n",
    "            tree = {question: [yes_answer, no_answer]}\n",
    "\n",
    "            return self.pruning_result(tree, df_train, df_val)\n",
    "\n",
    "\n",
    "def get_df(file):\n",
    "    # loading data into a dataframe\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # dropping unwanted columns from DF\n",
    "    cols_to_drop = ['PassengerId', 'Name', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
    "    df = df.drop(cols_to_drop, axis=1)\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    encoder = LabelEncoder()\n",
    "    df['Sex'] = encoder.fit_transform(df['Sex'])\n",
    "\n",
    "    # Since we have null values only for the Age column,we will replace\n",
    "    # the null values with the mean\n",
    "    df = df.fillna(df['Age'].mean())\n",
    "    df = df.rename(columns={\"Survived\": \"label\"})\n",
    "    df = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'label']]\n",
    "    return df\n",
    "\n",
    "def get_train_test_split_kfold(data_df, no_of_splits=5):\n",
    "    from random import shuffle\n",
    "    # get indices from data\n",
    "    indices = data_df.index.to_list()\n",
    "    shuffle(indices)\n",
    "    grouped_indices = [indices[i::no_of_splits] for i in range(no_of_splits)]\n",
    "    test_train_batches = []#{'batch-{}'.format(i + 1): None for i in range(no_of_splits)}\n",
    "    for j in range(no_of_splits):\n",
    "        test_df = data_df.iloc[grouped_indices[j]]\n",
    "        train_df = data_df.drop(grouped_indices[j])\n",
    "        # test_train_batches['batch-{}'.format(j + 1)] = [train_df, test_df]\n",
    "        test_train_batches.append((train_df, test_df))\n",
    "    return test_train_batches\n",
    "\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    file = r\".\\titanic.csv\"\n",
    "    data_df = get_df(file)\n",
    "    column_headers = data_df.columns\n",
    "    test_train_batches = get_train_test_split_kfold(data_df, no_of_splits=5)\n",
    "    trees = []\n",
    "    pruned_trees = []\n",
    "    accuracies = []\n",
    "    pruned_accuracies = []\n",
    "    best_accuracy, idx1 = 0, 0\n",
    "    from tqdm import tqdm\n",
    "    for i, data in tqdm(enumerate(test_train_batches)):\n",
    "        train, test = data\n",
    "        y_test = test.iloc[:,0]\n",
    "        tree = decision_tree(train, test, prune=False)\n",
    "        trees.append(tree)\n",
    "        # 1\n",
    "        # pruned_trees.append(tree.pruned_tree)\n",
    "        if tree.accuracy > best_accuracy:\n",
    "            best_accuracy = tree.accuracy\n",
    "            idx = i\n",
    "        accuracies.append(tree.accuracy)\n",
    "        # 1\n",
    "        # pruned_accuracies.append(tree.pruned_accuracy)\n",
    "\n",
    "    best_tree = trees[idx]\n",
    "    pruned_best_tree = best_tree.post_pruning(best_tree.tree, best_tree.train, best_tree.test)\n",
    "    pruned_accuracy = best_tree.calculate_accuracy(pruned_best_tree)\n",
    "\n",
    "    print('accuracies of Trees corresponding to 5 data splits :')\n",
    "    for i, accuracy in enumerate(accuracies):\n",
    "        print('tree-{} : {}'.format(i+1, round(accuracy, 3)))\n",
    "\n",
    "    # 1\n",
    "    # print('accuracies of Trees corresponding to 5 data splits after pruning:')\n",
    "    # for i, accuracy in enumerate(pruned_accuracies):\n",
    "    #     print('tree-{} : {}'.format(i + 1, round(accuracy, 3)))\n",
    "\n",
    "    # 2\n",
    "    print('tree with best accuracy - {} :'.format(round(best_accuracy, 4)))\n",
    "    pprint(trees[idx].tree)\n",
    "\n",
    "    # 1\n",
    "    # print('best tree with best accuracy - {} after pruning [pruned accuracy - {}] :'.format(round(best_accuracy, 2), round(pruned_accuracies[idx], 2)))\n",
    "    # pprint(pruned_trees[idx])\n",
    "\n",
    "    # 2\n",
    "    print('best tree with best accuracy - {} after pruning [pruned accuracy - {}] :'.format(round(best_accuracy, 2), round(pruned_accuracy, 2)))\n",
    "    pprint(pruned_best_tree)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4f6437a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1\n",
      " 1 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1\n",
      " 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0\n",
      " 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0\n",
      " 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1]\n",
      "79.7752808988764\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch']\n",
    "sk_tree = DecisionTreeClassifier()\n",
    "sk_tree.fit(train[features], train['label'])\n",
    "print(sk_tree.predict(test[features]))\n",
    "print(sk_tree.score(test[features], test['label'])*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
